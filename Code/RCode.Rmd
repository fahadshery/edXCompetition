# R Code for edX Competition on *"Analytics Edge"* DataScience Course
=====================================================================

#### Author: Fahad Usman
#### Start Date: 21 April 2014

## The Problem Statement: Learn what predicts happiness by using informal polling questions. 

What predicts happiness? In this competition, you'll be using data from Show of Hands, an informal polling platform for use on mobile devices and the web, to see what aspects and characteristics of people's lives predict happiness.

Show of Hands has been downloaded over 300,000 times across Apple and Android app stores, and users have cast more than 75 million votes. In this problem, we'll use data from thousands of users and one hundred different questions to see which responses predict happiness.

## Acknowledgements

This competition is brought to you by 15.071x, edX, and Show of Hands.

## File descriptions

Here is a description of the files you have been provided for this competition:

- **train.csv** - the training set of data that you should use to build your models
- **test.csv**  - the test set that you will be evaluated on. It contains all of the independent variables, but not the dependent variable.
- **sampleSubmission.csv** - a sample submission file in the correct format.
- **Questions.pdf** - the question test corresponding to each of the question codes, as well as the possible answers.

**Note:**  You might be wondering why the dependent variable is not there in the test set because that is what a test set is meant to be in the true sense. Your model might do well on the training data but the challenge is to make it flexible enough so that it can predict well enough on any unseen data.

*For example*, if you are predicting an election winner for the current election based on opinion polls you always do that before the actual outcome comes in i.e. you do not have a dependent variable in your test set(current election) but have the dependent variable in your train set(previous elections).

When you start working with predictions for the first time, you might be working with dependenable variables in both training and testing sets...you might be wondering why we should be given the dependent variable in test set then. That is probably done to make us understand the importance of the fact that what works well on a train set might not work so well on a test set.

## Data fields

- **UserID** - an anonymous id unique to a given user
- **YOB** - the year of birth of the user
- **Gender** - the gender of the user, either Male, Female, or not provided
- **Income** - the household income of the user. Either not provided, or one of "under $25,000", "$25,001 - $50,000", "$50,000 - $74,999", "$75,000 - $100,000", "$100,001 - $150,000", or "over $150,000".
- **HouseholdStatus** - the household status of the user. Either not provided, or one of "Domestic Partners (no kids)", "Domestic Partners (w/kids)", "Married (no kids)", "Married (w/kids)", "Single (no kids)", or "Single (w/kids)".
- **EducationLevel** - the education level of the user. Either not provided, or one of "Current K-12", "High School Diploma", "Current Undergraduate", "Associate's Degree", "Bachelor's Degree", "Master's Degree", or "Doctoral Degree".
- **Party** - the political party of the user. Either not provided, or one of "Democrat", "Republican", "Independent", "Libertarian", or "Other".
- **Happy** - a binary variable, with value 1 if the user said they were happy, and with value 0 if the user said that were neutral or not happy. This is the variable you are trying to predict.
- **Q124742, Q124122, . . . , Q96024** - 101 different questions that the users were asked on Show of Hands. If the user didn't answer the question, there is a blank. For information about the question text and possible answers, see the file Questions.pdf.
- **votes** - the total number of questions that the user responded to, out of the 101 questions included in the data set (this count does not include the happiness question).

**Note** - Dependent variable is ***Happy*** and is a binary variable. This is also classed as *categorical variable* which only takes two possible values either a person is happy (1) or not (0)...

## Linear Regression Vs Logistic Regression


In the **Linear regression** model the dependent variable y is considered continuous, whereas in **Logistic regression** it is categorical, i.e., discrete. In application, the linear regression is used in regression settings while the logistic regression is used for binary classification or multi-class classification (where it is called multinomial logistic regression). 

In other words:

**Linear regression** is regression when the X variable (the predictor or the independent variable) and the Y variable (the response or the dependent variable) are *both* continuous and linearly related, so the response will **increase or decrease** at a *constant ratio* to the *predictor*. you can also have more then one predictor, which give you regression in multiple dimensions, but in the end it comes down to a set increase/decrease in the response for every one unit increase in the predictor(s).

**Logistic regression** is quite different, the predictor/independent variable is continuous, but the response/dependent variable categorical or dichotomous (only 2 options). The logistic regression provides you with the probability of an event occurring. For every one unit increase in the predictor/independent variable, the probability of an even occurring increases/decreases. However what is actually observed is whether or not the event occurs, the logistics regression can only be tested by seeing whether it's predictions come true. 

We have seen linear regression as a way of predicting continuous outcomes. Of course, we can utilize linear regression to predict happiness here, but then we have to round the outcome to 0 or 1. Instead we will use the logistic regression, which is an extension of linear regression, to environments where the dependent variable is categorical. In our case, 0 or 1. to start with as explained above.

Let's see if we can build a logistic regression model in R to better predict happiness...To begin, read in the train file provided:

```{r, include=FALSE}
library(markdown)
library(knitr)
```

```{r}
Train = read.csv("C:/Users/Fahad/Documents/R Projects/edXCompetition/Data/train.csv")
Test = read.csv("C:/Users/Fahad/Documents/R Projects/edXCompetition/Data/test.csv")

```

Always check the data structure first, So we can take a look at the structure of our new data frame by using the str and summary functions:

```{r}
str(Train)
head(str(Train$UserID))
summary(Train)
summary(Train$Q124742)
summary(Train$Q114386)
summary(Train$Q115899)
```

So we have 4619 observations and 110 columns/variables in the training set and 1980 obs. with 109 columns in the test set.

Let's check how many poeple in the given population are happy and how many are not in the training set:

```{r}

table(Train$Happy)
prop.table(table(Train$Happy))

```

56.4% people are happy and 43.6% are not in the training set.

Let's check the male/female proportions now:

```{r}
table(Train$Gender)
# proportion of men, women and not answered in the train set
prop.table(table(Train$Gender))
```

Here we have 52.6% male, 35.7% female and 11.6% not provided population in the training set.

Let's check what proportion of these people are happy by the following command:

```{r}
table(Train$Gender, Train$Happy)
prop.table(table(Train$Gender, Train$Happy))

```

Here we see that for example 1387 males are happy and 1045 are not.. So the proportion of male population which are happy is about 57%. However, the proportion table command doesn't show that because by default it takes each entry in the table and divides by the total number of persons. What we want to see is the row-wise proportion, ie, the proportion of each sex that is happy, as separate groups. So we need to tell the command to give us proportions in the 1st dimension which stands for the rows (using '2' instead would give you column proportions):

```{r}
prop.table(table(Train$Gender, Train$Happy), 1)

```
OK, That's better! You can see male proportion who is happy is about 57%, females are about 54% and about 60% people are happy who didnt answer. You can also use tapply function which can show what proportion of Gender population is happy by:

```{r}
tapply(Train$Happy, Train$Gender, mean)
```

This now shows that 45.7% of all the females are not happy and 54.2% are happy. Similarly, 57% overall male population is happy and 43% is not happy. This shows that male population is a bit more happier than the female population and finally about 60% people who didn't supplied Gender information in the train dataset are happy and 40% are not.

Let's look into the income variable now (just following intuition tbh!):

```{r}
table(Train$Income, Train$Happy)
prop.table(table(Train$Income, Train$Happy),1)

```
Interestingly, 63.7% of the people earning between $100,001 - $150,000 are happy and 36.2% of people are not happy...62% people are happy earning over $150,000 and 36.2% of people are not happy are earning between $100,001 - $150,000. This category has the least amount of unhappy people.

## Baseline Model

Before building any model, let's consider a simple baseline model....we can compare our predictions to the baseline method of predicting the average/mean outcome for all data points. 

In a classification problem, a standard baseline method is to just predict the most frequent outcome for all observations. So to check the common outcome we can use the table command again:

```{r}

table(Train$Happy)

```

We can see 2604 out 4619 people are happy and 2015 are not happy...Since happiness is more common than unhappy, in this case, we would predict that everyone is happy.

```{r}
2604/nrow(Train)

```

If we did this, we would get 2604 out of the 4619 observations correct, or have an accuracy of about 56.4%. So our baseline model has an accuracy of 56.4%. This is what we'll try to beat with our logistic regression model.

As we know that we don't have dependent variable in the test set, So we want to randomly split our data set into a training set and testing set so that we'll have a test set to measure our out-of-sample accuracy.

**NOTE:**  You would require caTools package

```{r}
install.packages("caTools")
library(caTools)
```

Now, let's use this package to randomly split our data into a training set and testing set. We can split 70% data in training set and 30% in the test set, We can use sample.split() method of caTools library to do so:

```{r}

split = sample.split(Train$Happy, SplitRatio=0.7)

```

Sample.split randomly splits the data. But it also makes sure that the outcome variable is well-balanced in each piece. We saw earlier that 56.4% people are happy and 43.6% are unhappy. This function makes sure that in our training set, 56.4% of our population is happy and in our testing set 56.4% of our population
is happy too. We want to do this so that our test set is representative of our training set. split variable we just created has TRUE or FALSE values only... True means put them in training set and false means put those datapoints into test set. Now let's subset the data:

```{r}

splitTrain = subset(Train, split==TRUE)
splitTest = subset(Train, split==FALSE)

```

## Logistic Regression Model with 3 Variables only

Now, we are ready to build a logistic regression model using three variables Income, HouseholdStatus and EducationLevel as independent variables.

```{r}

HappyLogRegModel = glm(Happy ~ Income + HouseholdStatus + EducationLevel, data = splitTrain, family=binomial)

```

family=binomial argument tells the glm function to build a logistic regression model.

Now, let's look at our model using the summary function.

```{r}

summary(HappyLogRegModel)

```
Apart from Income$25,001 - $50,000, HouseholdStatusMarried (no kids), HouseholdStatusMarried (w/kids), HouseholdStatusSingle (w/kids) Almost all the other variables are insignificant. We see here that the coefficients for HouseholdStatusMarried (no kids) and HouseholdStatusMarried (w/kids) are +ve which means that higher values in these two variables are indicative of happiness. The last thing we want to look at in the output is the AIC value. This is a measure of the quality of the model and is like Adjusted R-squared in that it accounts for the number of variables used compared to the number of observations. Unfortunately, it can only be compared between models on the same data set. But it provides a means for model selection. The preferred model is the one with the minimum AIC.

So let's make our first prediction on the splitTest dataset by:

```{r}

predictHappiness = predict(HappyLogRegModel, newdata=splitTest, type="response")

```

Let's take a look at the statistical summary of our predictions.

```{r}

summary(predictHappiness)

```

From summary above, we saw that outcome of the logistic regression model is a probability. Since we're expecting probabilities, all of the numbers should be between zero and one. And we see that the minimum value is about 0.33 and the maximum value is 0.7792. Let's see if we're predicting higher probabilities for the actual happy cases as we expect. To do this, use the tapply function, giving as arguments predictHappiness and then splitTest$Happy and then mean.

```{r}

tapply(predictHappiness, splitTest$Happy, mean)

```
This will compute the average prediction for each of the true outcomes. So we see that for all of the true happy cases, we predict an average probability of about 0.58. And all of the true unhappy cases, we predict an average probability of about 0.55. So this is a good sign, because it looks like we're predicting a higher probability for the actual happy cases.

Often, we want to make an actual prediction. Should we predict 1 for happy case, or should we predict 0 for unhappy case? We can convert the probabilities to predictions using what's called a **threshold value, t**.

If the probability of happy case is greater than this threshold value, t, we predict happy case. But if the probability of happy case is less than the threshold value, t, then we predict unhappy case. But what value should we pick for the threshold, t? The threshold value, t, is often selected based on which errors are better. You might be thinking that making no errors is better, which is, of course, true. But it's rare to have a model that predicts perfectly, so you're bound to make some errors.

There are two types of errors that a model can make -- ones where you predict 1, or happy case, but the actual outcome is 0, and ones where you predict 0, or unhappy case, but the actual outcome is 1 or happy case. If we pick a large threshold value t, then we will predict happy case rarely, since the probability of happy case has to be really large to be greater than the threshold. This means that we will make more errors where we say unhappy case, but it's actually happy case. This approach would detect the people who are really happy. On the other hand, if the threshold value, t, is small, we predict happy case frequently, and we predict unhappy case rarely. This means that we will make more errors where we say happy case, but it's actually unhappy case. This approach would detect all people who might be happy cases. 

Some decision-makers often have a preference for one type of error over the other, which should influence the threshold value they pick. If there's no preference between the errors, the right threshold to select is t = 0.5, since it just predicts the most likely outcome. To make this discussion a little more quantitative, we use what's called a **confusion matrix or classification** matrix.

We can compute two outcome measures that help us determine what types of errors we are making. They're called **sensitivity and specificity**.
*Sensitivity* is equal to the true positives divided by the true positives plus the false negatives, and measures the percentage of actual happy cases that we classify correctly. This is often called the true positive rate.

*Specificity* is equal to the true negatives divided by the true negatives plus the false positives, and measures the percentage of actual unhappy cases that we classify correctly. This is often called the true negative rate. 

A model with a higher threshold will have a lower sensitivity and a higher specificity.
A model with a lower threshold will have a higher sensitivity and a lower specificity.

Let's create the confusion matrix by table command:

```{r}

table(splitTest$Happy, predictHappiness > 0.5)

```

This will return TRUE if our prediction is greater than 0.5, which means we want to predict happy case, and it will return FALSE if our prediction is less than 0.5, which means we want to predict unhappy case.

Let's compute the sensitivity, or the true positive rate, and the specificity, or the true negative rate.

```{r}

sensitivity = 625/(625+156)
specificity = 172/(172+433)

```
The above Sensitivity and specificity are calculated at 0.5 threshold value. But which threshold should we pick? we'll see a nice visualization to help us select a threshold.

A Receiver Operator Characteristic curve, or ROC curve, can help you decide which value of the threshold is best. To generate these ROC curves, you need ROCR package.

```{r}
install.packages("ROCR")
library(ROCR)
```

Recall that we made predictions on our splitTest and called them predictHappiness. We'll use these predictions to create our ROC curve. Use the prediction function if ROCR package:

```{r}

ROCRpredHappiness = prediction(predictHappiness, splitTest$Happy)

```
This function takes two arguments, first the prediction variable we created when we were predicting and the second one is the actual outcome.

Now, we need to use the performance function. This defines what we'd like to plot on the x and y-axes of our ROC curve. This function takes the ROCR predection and then what we want to plot on x-axis (in our case we want to plot true +ve rate) and what we want to plot on the y-axis (in our case we want to plot False +ve rate)

```{r}
ROCRperfHappiness = performance(ROCRpredHappiness, "tpr", "fpr")

```

Now plot ROCRperfHappiness

```{r}
plot(ROCRperfHappiness)
```

The sensitivity, or true positive rate of the model, is shown on the y-axis. And the false positive rate, or 1 minus the specificity, is given on the x-axis. The line shows how these two outcome measures vary with different threshold values.

The ROC curve always starts at the point (0, 0). This corresponds to a threshold value of 1. If you have a threshold of 1, you will not catch any happy cases, or have a sensitivity of 0. But you will correctly label of all the unhappy 
cases, meaning you have a false positive rate of 0.

The ROC curve always ends at the point (1,1), which corresponds to a threshold value of 0. If you have a threshold of 0, you'll catch all of the happy cases, or have a sensitivity of 1, but you'll label all of the unhappy cases as happy cases too, meaning you have a false positive rate of 1.

The threshold decreases as you move from (0,0) to (1,1). At the point (0.25, 0.4), you're correctly labeling about 40% of the happy cases with a very small false positive rate.

On the other hand, at the point (0.8, 0.9), you're correctly labeling about 90% of the happy cases, but have a false positive rate of 80%.

In the middle, around (0.5, 0.6), you're correctly labeling about 60% of the happy cases, with a 50% false positive rate.

The ROC curve captures all thresholds simultaneously. The higher the threshold, or closer to (0, 0), the higher the specificity and the lower the sensitivity. The lower the threshold, or closer to (1,1), the higher the sensitivity and lower the specificity. **So which threshold value should you pick?** You should select the best threshold for the trade-off you want to make.

If you're more concerned with having a high specificity or low false positive rate, pick the threshold that maximizes the true positive rate while keeping the false positive rate really low. A threshold around (0.59, 0.7) on this ROC curve looks like a good choice in this case.

On the other hand, if you're more concerned with having a high sensitivity or high true positive rate, pick a threshold that minimizes the false positive rate but has a very high true positive rate.

Now, you can add colors by adding one additional argument to the plot function. print.cutoffs.at=seq(0,1,0.05), which will print the threshold values in increments of 0.05. If you want finer increments, just decrease the value of 0.05.

```{r}
plot(ROCRperfHappiness, colorize=TRUE, print.cutoffs.at=seq(0,1,0.05), text.adj=c(-0.2,1.7))

```
Using this curve, we can determine which threshold value we want to use depending on our preferences as a decision-maker..

We can now calculate the accuracy of our model by:

```{r}
table(splitTest$Happy, predictHappiness > 0.5)
(625+172)/nrow(splitTest)
(625+172)/(172+433+156+625)

```

The accuracy of our model is about 57.5%

## First Submission on Kaggle

We are now ready for our first Kaggle submission. For this we need to create the model on the Train set (which is provided by the staff of MIT) and predict on the Test set (again, provided by the staff team)

```{r}

HappyLogRegModel1 = glm(Happy ~ Income + HouseholdStatus + EducationLevel, data = Train, family=binomial)

```

predict on the test set now:


```{r}

predictHappiness1 = predict(HappyLogRegModel1, newdata=Test, type="response")

```

Look at the summary:

```{r}

summary(predictHappiness1)
```
Now that we have made our predictions, we are ready to submit on Kaggle. But first, we need to prepare the data file in the csv format which we can do by:

```{r}
#first check the sample provided by the Staff
sample = read.csv("sampleSubmission.csv")
View(sample)
#we need to bind userids with our predicted probabilities
submission = cbind(UserID=Test$UserID, Probability1=predictHappiness1)
#check how it looks
View(submission)
#time to write this dataframe into the csv format by:
write.table(submission,file="submission1.csv",sep=",",row.names=F)
```

Now go to Kaggle and submit this csv file and WOLLAH!!!

## Linear Regression

Linear regression discards observations with missing data, so we will remove all such observations from the training and testing sets. Later, we will learn about imputation, which deals with missing data by filling in missing values with plausible information.

The following commands can be used to remove observations with any missing value from happinessTrain and happinessTest:

```{r}
hap.Train.Without.Missing.Values = na.omit(happinessTrain)
hap.Test.Without.Missing.Values = na.omit(happinessTest)
```

check the structure now:

```{r}
str(hap.Train.Without.Missing.Values)
summary(hap.Train.Without.Missing.Values)
```
Let's build our first Linear Regression model using all variables except the UserID, YOB and votes:

```{r}

LinearRegModel = lm(Happy ~ . -UserID -YOB -votes,data=hap.Train.Without.Missing.Values)
summary(LinearRegModel)

```

The training-set RMSE can be computed by first computing the SSE:
```{r}
SSE = sum(LinearRegModel$residuals^2)
```
and then dividing by the number of observations and taking the square root:
```{r}
RMSE = sqrt(SSE / nrow(hap.Train.Without.Missing.Values))
```
A alternative way of getting this answer would be with the following command:
```{r}
sqrt(mean(LinearRegModel$residuals^2)) 
```

We can now try removing insignificant variables one by one to see if we can improve our model.  When we remove insignificant variables, the "Multiple R-squared" will always be worse, but only slightly worse. This is due to the nature of a linear regression model. It is always possible for the regression model to make a coefficient zero, which would be the same as removing the variable from the model. The fact that the coefficient is not zero in the intial model means it must be helping the R-squared value, even if it is only a very small improvement. So when we force the variable to be removed, it will decrease the R-squared a little bit. However, this small decrease is worth it to have a simpler model.

On the contrary, when we remove insignificant variables, the "Adjusted R-squred" will frequently be better. This value accounts for the complexity of the model, and thus tends to increase as insignificant variables are removed, and decrease as insignificant variables are added.


## Automatically Building the Model

We have many variables in this problem, and many are insignificant...R provides a function, **step**, that will automate the procedure of trying different combinations of variables to find a good compromise of model simplicity and R2. This trade-off is formalised by the Akaike information criterion (AIC) - it can be informally thought of as the quality of the model with a penalty for the number of variables in the model.

The step function has one argument - the name of the initial model. It returns a simplified model. Use the step function in R to derive a new model, with the full model as the initial model.

```{r}
LinearRegStepModel = step(LinearRegModel)
summary(LinearRegStepModel)
```

It is interesting to note that the step function does not address the collinearity of the variables, except that adding highly correlated variables will not improve the R2 significantly. The consequence of this is that the step function will not necessarily produce a very interpretable model - just a model that has balanced quality and simplicity for a particular weighting of quality and simplicity (AIC).

